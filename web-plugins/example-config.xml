<IQser>
	<version>1.5</version>
	<index>
		<path>./index</path>
	</index>

	<!-- ServiceLocator -->
	<servicelocator>
		<class>com.iqser.core.ejb.util.JBossServiceLocator</class>
	</servicelocator>

	<!-- CategoryBuilder -->
	<category>
		<class>com.iqser.core.category.DefaultCategoryBuilder</class>
		<!-- Run every day at 23:00. every 15min for testing only -->
		<scheduler>0 0 23 * * ?</scheduler>
	</category>

	<!-- SecurityManager -->
	<securitymanager>
		<class>
			com.iqser.core.security.basic.BasicSecurityManager
		</class>
	</securitymanager>

	<!-- Analyzer -->
	<analyzer>
		<task>
			<name>SyntaxAnalyzer</name>
			<class>
				com.iqser.core.analyzer.task.syntax.SyntaxAnalyzerTask
			</class>
		</task>
		<task>
			<name>SemanticAnalyzer</name>
			<class>
				com.iqser.core.analyzer.task.semantic.SemanticAnalyzerTask
			</class>
		</task>
		<task>
			<name>PatternAnalyzer</name>
			<class>
				com.iqser.core.analyzer.task.pattern.PatternAnalyzerTask
			</class>
		</task>
	</analyzer>
	<plugins>
		<!-- Plugin settings are ignored for testing -->
		<plugin>
			<id>com.iqser.plugin.web.festo</id>
			<!-- Modify the type in your iqser configuration, if necessary -->
			<type>Festo Product Information</type>
			<name>PDF Web Plugin</name>
			<vendor>IQser Technologies</vendor>
			<provider-class>com.iqser.demo.plugin.web.PDFContentProvider</provider-class>
			<!-- Use a Cron formatted string to define the synchronisation schedule. -->
			<scheduler> 
				<syncjob>0 0 0 * * ?</syncjob> 
				<housekeeperjob>0 0 23 * * ?</housekeeperjob> 
			</scheduler> 
			<!-- Crawler settings -->
			<init-param>
				<!-- Required server as a starting point for crawling -->
				<param-name>start-server</param-name>
				<param-value>http://www.festo.com</param-value>	
			</init-param>
			<init-param>
				<!-- Optional path as a starting point for crawling -->
				<param-name>start-path</param-name>
				<param-value>/net/de_de/SupportPortal/default.aspx</param-value>
			</init-param>
			<init-param>
				<!-- Optional filter to limit the crawler to a domain -->
				<param-name>server-filter</param-name>
				<param-value>http://www.festo.co</param-value>
			</init-param>
			<init-param>
				<!-- Optional filter to limit the crawler to a path -->
				<param-name>path-filter</param-name>
				<param-value>/net/</param-value>
			</init-param>
			<init-param>
				<!-- Optional filter to define the links to crawl -->
				<param-name>link-filter</param-name>
				<param-value>(.*\\.pdf$)||(.*\\.PDF$)||(.*cat=\\d*$)</param-value>
			</init-param>
			<init-param>
				<!-- Optional filter to specify the crawl depth. Default is 2 -->
				<param-name>maxdepth-filter</param-name>
				<param-value>6</param-value>
			</init-param>
			<init-param>
				<!-- Optional filter to specify items to parse. Default are web pages -->
				<param-name>item-filter</param-name>
				<param-value>(.*\\.pdf$)||(.*\\.PDF$)</param-value>
			</init-param>
			<!-- Database settings -->
			<init-param>
				<param-name>database</param-name>
				<param-value>localhost/crawler</param-value>
			</init-param>
			<init-param>
				<param-name>username</param-name>
				<param-value>master</param-value>
			</init-param>	
			<init-param>
				<param-name>password</param-name>
				<param-value>root</param-value>
			</init-param>
		</plugin>
		<plugin>
			<id>net.sf.iqser.plugin.web.html</id>
			<!-- Modify the type in your iqser configuration, if necessary -->
			<type>Custom Tariff</type>
			<name>Custom Web Plugin</name>
			<vendor>IQser Technologies</vendor>
			<provider-class>
				net.sf.iqser.plugin.web.html.HTMLContentProvider
			</provider-class>
			<!-- Use a Cron formatted string to define the synchronisation schedule. -->
			<scheduler> 
				<syncjob>0 0 3 * * ?</syncjob> 
				<housekeeperjob>0 0 23 * * ?</housekeeperjob> 
			</scheduler> 
			<!-- Parser settings -->
			<init-param>
				<param-name>item-node-filter</param-name>
				<param-value>html,*,*,*</param-value>
			</init-param>
			<init-param>
				<param-name>attribute-node-filter</param-name>
				<param-value>title,*,*,*;meta,*,*,*</param-value>
			</init-param>
			<init-param>
				<param-name>key-attributes</param-name>
				<param-value>[Name] [Description] [Keywords]</param-value>
			</init-param>
			<!-- Mappings -->
			<init-param>
				<param-name>TITLE</param-name>
				<param-value>Name</param-value>
			</init-param>
			<init-param>
				<param-name>description</param-name>
				<param-value>Description</param-value>
			</init-param>
			<init-param>
				<param-name>keywords</param-name>
				<param-value>Keywords</param-value>
			</init-param>
			<!-- Crawler settings -->
			<init-param>
				<!-- Required server as a starting point for crawling -->
				<param-name>start-server</param-name>
				<param-value>http://www.zolltarifnummern.de</param-value>	
			</init-param>
			<init-param>
				<!-- Optional path as a starting point for crawling -->
				<param-name>start-path</param-name>
				<param-value>/?year=2010&lang=de&q=1*</param-value>
			</init-param>
			<init-param>
				<!-- Optional filter to limit the crawler to a domain -->
				<param-name>server-filter</param-name>
				<param-value>http://www.zolltarifnummern.de</param-value>
			</init-param>
			<init-param>
				<!-- Optional filter to limit the crawler to a path -->
				<param-name>path-filter</param-name>
				<param-value>/2010_de/</param-value>
			</init-param>
			<init-param>
				<!-- Optional filter to define the links to crawl -->
				<param-name>link-filter</param-name>
				<param-value>(.*\\.html$)||(.*\\.html$)</param-value>
			</init-param>
			<init-param>
				<!-- Optional filter to specify the crawl depth. Default is 2 -->
				<param-name>maxdepth-filter</param-name>
				<param-value>2</param-value>
			</init-param>
			<init-param>
				<!-- Optional filter to specify items to parse. Default are web pages -->
				<param-name>item-filter</param-name>
				<param-value>.*/\\d{8}.html$</param-value>
			</init-param>
			<!-- Database settings -->
			<init-param>
				<param-name>database</param-name>
				<param-value>localhost/crawler</param-value>
			</init-param>
			<init-param>
				<param-name>username</param-name>
				<param-value>master</param-value>
			</init-param>	
			<init-param>
				<param-name>password</param-name>
				<param-value>root</param-value>
			</init-param>
		</plugin>
		<plugin>
			<id>com.iqser.plugin.web.wlw</id>
			<!-- Modify the type in your iqser configuration, if necessary -->
			<type>Vendor</type>
			<name>WlW Web Plugin</name>
			<vendor>IQser Technologies</vendor>
			<provider-class>
				net.sf.iqser.plugin.web.html.HTMLContentProvider
			</provider-class>
			<!-- Use a Cron formatted string to define the synchronisation schedule. -->
			<scheduler> 
				<syncjob>0 0 5 * * ?</syncjob> 
				<housekeeperjob>0 0 23 * * ?</housekeeperjob> 
			</scheduler> 
			<!-- Parser settings -->
			<init-param>
				<param-name>item-node-filter</param-name>
				<param-value>*,class,subcontentFirmeninformation,*</param-value>
			</init-param>
			<init-param>
				<param-name>attribute-node-filter</param-name>
				<param-value>*,class,firmierung,*;tr,*,*,*;*,class,ansprechpartner,*</param-value>
			</init-param>
			<init-param>
				<param-name>key-attributes</param-name>
				<param-value>[Name] [Description] [Contact]</param-value>
			</init-param>
			<!-- Mappings -->
			<init-param>
				<param-name>firmierung</param-name>
				<param-value>Name</param-value>
			</init-param>
			<init-param>
				<param-name>ansprechpartner</param-name>
				<param-value>Description</param-value>
			</init-param>
			<init-param>
				<param-name>TR</param-name>
				<param-value>Address</param-value>
			</init-param>
			<init-param>
				<param-name>charset</param-name>
				<param-value>UTF-8</param-value>
			</init-param>
			<!-- Crawler settings -->
			<init-param>
				<!-- Required server as a starting point for crawling -->
				<param-name>start-server</param-name>
				<param-value>http://www.wlw.de</param-value>	
			</init-param>
			<init-param>
				<!-- Optional path as a starting point for crawling -->
				<param-name>start-path</param-name>
				<param-value>/sse/MainServlet?sprache=de&land=DE&anzeige=produkt&suchbegriff=Maschinen</param-value>
			</init-param>
			<init-param>
				<!-- Optional filter to limit the crawler to a domain -->
				<param-name>server-filter</param-name>
				<param-value>http://www.wlw.de</param-value>
			</init-param>
			<init-param>
				<!-- Optional filter to limit the crawler to a path -->
				<param-name>path-filter</param-name>
				<param-value>/sse/</param-value>
			</init-param>
			<init-param>
				<!-- Optional filter to define the links to crawl -->
				<param-name>link-filter</param-name>
				<param-value>((\\S*MainServlet\\Sanzeige=kurzliste\\S*$)||(\\S*MainServlet\\Sanzeige=vollanzeige\\S*$)</param-value>
			</init-param>
			<init-param>
				<!-- Optional filter to specify the crawl depth. Default is 2 -->
				<param-name>maxdepth-filter</param-name>
				<param-value>2</param-value>
			</init-param>
			<init-param>
				<!-- Optional filter to specify items to parse. Default are web pages -->
				<param-name>item-filter</param-name>
				<param-value>\\S*MainServlet\\Sanzeige=vollanzeige\\S*$</param-value>
			</init-param>
			<!-- Database settings -->
			<init-param>
				<param-name>database</param-name>
				<param-value>localhost/crawler</param-value>
			</init-param>
			<init-param>
				<param-name>username</param-name>
				<param-value>master</param-value>
			</init-param>	
			<init-param>
				<param-name>password</param-name>
				<param-value>root</param-value>
			</init-param>
		</plugin>
	</plugins>
</IQser>